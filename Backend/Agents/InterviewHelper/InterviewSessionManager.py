"""
ðŸŽ¥ REAL-TIME INTERVIEW SESSION MANAGER
Live Video Analysis & Interactive AI Interview Experience

This module manages:
- Live camera feed processing and analysis
- Real-time behavioral feedback display
- Interactive AI interviewer conversation
- Session recording and performance tracking
- Dynamic UI updates during interviews
"""

import cv2
import threading
import queue
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Callable
import numpy as np
from dataclasses import dataclass, asdict
from pathlib import Path
import speech_recognition as sr

from .AIInterviewEngine import (
    AIInterviewEngine, InterviewSession, InterviewQuestion, InterviewResponse,
    VideoAnalysisResult, AudioAnalysisResult, InterviewType, InterviewDifficulty,
    camera_analyzer, audio_analyzer, ai_interview_engine
)

@dataclass
class LiveAnalysisData:
    """Real-time analysis data for UI updates"""
    eye_contact_score: float
    confidence_level: float
    professionalism_score: float
    speaking_pace: float
    filler_word_count: int
    current_question: str
    time_remaining: int
    overall_performance: float
    recommendations: List[str]

@dataclass
class SessionSettings:
    """Interview session configuration"""
    show_live_feedback: bool = True
    record_session: bool = True
    enable_hints: bool = True
    time_limit_per_question: int = 300  # 5 minutes
    show_confidence_meter: bool = True
    enable_voice_commands: bool = True
    background_analysis: bool = True

class InterviewSessionManager:
    """
    Manages live interview sessions with real-time analysis
    
    Features:
    - Live camera feed with overlay analysis
    - Real-time behavioral coaching
    - Interactive AI interviewer
    - Session recording and playback
    - Performance metrics display
    """
    
    def __init__(self):
        self.engine = ai_interview_engine
        self.active_session: Optional[InterviewSession] = None
        self.session_thread: Optional[threading.Thread] = None
        self.analysis_thread: Optional[threading.Thread] = None
        
        # Real-time data queues
        self.video_queue = queue.Queue(maxsize=30)
        self.audio_queue = queue.Queue(maxsize=10)
        self.analysis_queue = queue.Queue(maxsize=100)
        
        # Session control flags
        self.session_running = False
        self.analysis_running = False
        self.recording_enabled = False
        
        # Live analysis data
        self.current_analysis = LiveAnalysisData(
            eye_contact_score=0.0,
            confidence_level=0.0,
            professionalism_score=0.0,
            speaking_pace=0.0,
            filler_word_count=0,
            current_question="",
            time_remaining=0,
            overall_performance=0.0,
            recommendations=[]
        )
        
        # Session settings
        self.settings = SessionSettings()
        
        # Video recording
        self.video_writer = None
        self.recording_start_time = None
        
        # Performance tracking
        self.performance_history: List[Dict] = []
        
        # Initialize speech recognition
        self.recognizer = sr.Recognizer()
        self.microphone = None
        try:
            self.microphone = sr.Microphone()
            with self.microphone as source:
                self.recognizer.adjust_for_ambient_noise(source)
        except:
            print("âš ï¸ Microphone not available")
        
        print("ðŸŽ¬ Interview Session Manager initialized!")
    
    def start_interview_session(self, 
                              session_id: str,
                              settings: Optional[SessionSettings] = None) -> bool:
        """Start a live interview session with real-time analysis"""
        
        if session_id not in self.engine.active_sessions:
            print(f"âŒ Session {session_id} not found")
            return False
        
        self.active_session = self.engine.active_sessions[session_id]
        
        if settings:
            self.settings = settings
        
        # Initialize camera
        if not self.engine.camera_active:
            if not self.engine.start_camera():
                print("âŒ Cannot start session without camera")
                return False
        
        # Start recording if enabled
        if self.settings.record_session:
            self._start_recording()
        
        # Start session threads
        self.session_running = True
        self.analysis_running = True
        
        self.session_thread = threading.Thread(target=self._run_interview_session, daemon=True)
        self.analysis_thread = threading.Thread(target=self._run_live_analysis, daemon=True)
        
        self.session_thread.start()
        self.analysis_thread.start()
        
        print(f"ðŸŽ¬ Started interview session: {session_id}")
        print(f"ðŸ¢ Company: {self.active_session.company_name}")
        print(f"ðŸ’¼ Role: {self.active_session.role}")
        
        # Welcome message from AI interviewer
        welcome_msg = f"Hello! Welcome to your interview for the {self.active_session.role} position at {self.active_session.company_name}. I'm your AI interviewer, and I'll be conducting this interview today. Let's begin!"
        self.engine.speak(welcome_msg)
        
        return True
    
    def stop_interview_session(self) -> Dict:
        """Stop the current interview session and return results"""
        
        if not self.active_session:
            return {}
        
        # Stop threads
        self.session_running = False
        self.analysis_running = False
        
        # Wait for threads to complete
        if self.session_thread and self.session_thread.is_alive():
            self.session_thread.join(timeout=5)
        
        if self.analysis_thread and self.analysis_thread.is_alive():
            self.analysis_thread.join(timeout=5)
        
        # Stop recording
        if self.recording_enabled:
            self._stop_recording()
        
        # Finalize session
        self.active_session.end_time = datetime.now()
        self.active_session.is_active = False
        
        # Calculate final scores
        final_results = self._calculate_final_results()
        
        # Save session results
        self._save_session_results(final_results)
        
        print("ðŸŽ¬ Interview session completed!")
        print(f"ðŸ“Š Overall Score: {final_results.get('overall_score', 0):.1f}%")
        
        # Reset session state
        session_results = final_results.copy()
        self.active_session = None
        
        return session_results
    
    def get_live_analysis(self) -> LiveAnalysisData:
        """Get current live analysis data for UI updates"""
        return self.current_analysis
    
    def process_voice_command(self, command: str) -> bool:
        """Process voice commands during interview"""
        
        command = command.lower().strip()
        
        if "pause" in command or "stop" in command:
            self._pause_interview()
            return True
        elif "resume" in command or "continue" in command:
            self._resume_interview()
            return True
        elif "repeat" in command or "say again" in command:
            self._repeat_current_question()
            return True
        elif "next" in command or "skip" in command:
            self._next_question()
            return True
        elif "help" in command:
            self._provide_help()
            return True
        
        return False
    
    def _run_interview_session(self):
        """Main interview session loop"""
        
        if not self.active_session:
            return
        
        try:
            # Introduction phase
            self._conduct_introduction()
            
            # Main interview questions
            for i, question in enumerate(self.active_session.questions):
                if not self.session_running:
                    break
                
                self.active_session.current_question_index = i
                self.current_analysis.current_question = question.question_text
                
                # Ask question
                self._ask_question(question)
                
                # Wait for and record answer
                response = self._record_answer(question)
                
                if response:
                    self.active_session.responses.append(response)
                    
                    # Provide immediate feedback if enabled
                    if self.settings.show_live_feedback:
                        self._provide_immediate_feedback(response)
                
                # Brief pause between questions
                time.sleep(2)
            
            # Closing phase
            self._conduct_closing()
            
        except Exception as e:
            print(f"âŒ Session error: {e}")
        
        finally:
            self.session_running = False
    
    def _run_live_analysis(self):
        """Continuous analysis of video and audio"""
        
        frame_count = 0
        last_analysis_time = time.time()
        
        while self.analysis_running:
            try:
                # Capture frame
                if self.engine.camera and self.engine.camera_active:
                    ret, frame = self.engine.camera.read()
                    
                    if ret:
                        frame_count += 1
                        
                        # Analyze every 10th frame to reduce load
                        if frame_count % 10 == 0:
                            # Video analysis
                            video_analysis = camera_analyzer.analyze_frame(frame)
                            
                            # Update live analysis data
                            self._update_live_analysis(video_analysis)
                            
                            # Add to analysis queue
                            if not self.analysis_queue.full():
                                self.analysis_queue.put({
                                    'timestamp': datetime.now(),
                                    'type': 'video',
                                    'data': video_analysis
                                })
                            
                            # Save frame for recording
                            if self.recording_enabled and not self.video_queue.full():
                                self.video_queue.put(frame.copy())
                        
                        # Display frame with overlay (in real app, send to UI)
                        if self.settings.show_live_feedback:
                            annotated_frame = self._add_analysis_overlay(frame)
                            cv2.imshow("Interview Analysis", annotated_frame)
                            
                            if cv2.waitKey(1) & 0xFF == ord('q'):
                                break
                
                # Brief pause to prevent overwhelming the system
                time.sleep(0.1)
                
            except Exception as e:
                print(f"âš ï¸ Analysis error: {e}")
                time.sleep(1)
    
    def _conduct_introduction(self):
        """Conduct interview introduction"""
        
        intro_questions = [
            "Before we begin, could you please introduce yourself?",
            "What interests you most about this role?",
            "What do you know about our company?"
        ]
        
        for intro_q in intro_questions:
            if not self.session_running:
                break
            
            self.engine.speak(intro_q)
            self.current_analysis.current_question = intro_q
            
            # Give time for response
            time.sleep(30)  # 30 seconds for intro questions
    
    def _ask_question(self, question: InterviewQuestion):
        """Ask interview question with timing"""
        
        # Speak the question
        self.engine.speak(question.question_text)
        
        # Update display
        self.current_analysis.current_question = question.question_text
        self.current_analysis.time_remaining = question.expected_duration
        
        print(f"\nðŸ¤– Question {self.active_session.current_question_index + 1}: {question.question_text}")
        
        if question.evaluation_criteria:
            print(f"ðŸ’¡ Focus areas: {', '.join(question.evaluation_criteria)}")
    
    def _record_answer(self, question: InterviewQuestion) -> Optional[InterviewResponse]:
        """Record and analyze user's answer"""
        
        start_time = datetime.now()
        response_text = ""
        
        # Start listening for audio
        audio_analysis = None
        video_analyses = []
        
        # Listen for the duration of the question
        end_time = start_time + timedelta(seconds=question.expected_duration)
        
        print(f"ðŸŽ¤ Recording answer... ({question.expected_duration} seconds)")
        
        while datetime.now() < end_time and self.session_running:
            # Update time remaining
            remaining = (end_time - datetime.now()).total_seconds()
            self.current_analysis.time_remaining = max(0, int(remaining))
            
            # Collect video analyses
            try:
                analysis_data = self.analysis_queue.get(timeout=1)
                if analysis_data['type'] == 'video':
                    video_analyses.append(analysis_data['data'])
            except queue.Empty:
                pass
            
            # Warning when time is running out
            if remaining <= 30 and remaining > 25:
                print("â° 30 seconds remaining...")
                if self.settings.enable_hints:
                    self.engine.speak("You have 30 seconds remaining for this question.")
            
            time.sleep(1)
        
        # Try to capture speech (simplified for demo)
        if self.microphone:
            try:
                with self.microphone as source:
                    print("ðŸŽ¤ Listening for final answer...")
                    audio = self.recognizer.listen(source, timeout=5, phrase_time_limit=10)
                    response_text = self.recognizer.recognize_google(audio)
                    print(f"ðŸ“ Captured: {response_text}")
            except:
                response_text = "Audio capture not available in demo mode"
        
        # Create response with analysis
        if video_analyses:
            # Average the video analyses
            avg_video_analysis = self._average_video_analyses(video_analyses)
            
            # Create audio analysis (mock for demo)
            audio_analysis = AudioAnalysisResult(
                timestamp=datetime.now(),
                speech_clarity=75.0,
                speaking_pace=120.0,
                voice_confidence=70.0,
                filler_words_count=2,
                volume_level=80.0,
                tone_analysis="professional",
                response_time=3.5
            )
            
            response = InterviewResponse(
                question_id=question.id,
                text_response=response_text,
                video_analysis=avg_video_analysis,
                audio_analysis=audio_analysis,
                response_duration=(datetime.now() - start_time).total_seconds(),
                quality_score=self._calculate_response_quality(avg_video_analysis, audio_analysis, response_text)
            )
            
            return response
        
        return None
    
    def _average_video_analyses(self, analyses: List[VideoAnalysisResult]) -> VideoAnalysisResult:
        """Average multiple video analysis results"""
        
        if not analyses:
            return VideoAnalysisResult(
                timestamp=datetime.now(),
                eye_contact_score=50.0,
                confidence_score=50.0,
                posture_score=50.0,
                gesture_appropriateness=50.0,
                facial_expression="neutral",
                attention_level=50.0,
                professionalism_score=50.0
            )
        
        avg_eye_contact = sum(a.eye_contact_score for a in analyses) / len(analyses)
        avg_confidence = sum(a.confidence_score for a in analyses) / len(analyses)
        avg_posture = sum(a.posture_score for a in analyses) / len(analyses)
        avg_gestures = sum(a.gesture_appropriateness for a in analyses) / len(analyses)
        avg_attention = sum(a.attention_level for a in analyses) / len(analyses)
        avg_professionalism = sum(a.professionalism_score for a in analyses) / len(analyses)
        
        return VideoAnalysisResult(
            timestamp=datetime.now(),
            eye_contact_score=avg_eye_contact,
            confidence_score=avg_confidence,
            posture_score=avg_posture,
            gesture_appropriateness=avg_gestures,
            facial_expression=analyses[-1].facial_expression,  # Use last expression
            attention_level=avg_attention,
            professionalism_score=avg_professionalism
        )
    
    def _calculate_response_quality(self, 
                                  video_analysis: VideoAnalysisResult,
                                  audio_analysis: AudioAnalysisResult,
                                  text: str) -> float:
        """Calculate overall response quality score"""
        
        # Video metrics (40% weight)
        video_score = (
            video_analysis.eye_contact_score * 0.3 +
            video_analysis.confidence_score * 0.4 +
            video_analysis.professionalism_score * 0.3
        ) * 0.4
        
        # Audio metrics (30% weight)  
        audio_score = (
            audio_analysis.speech_clarity * 0.4 +
            audio_analysis.voice_confidence * 0.3 +
            max(0, 100 - audio_analysis.filler_words_count * 5) * 0.3
        ) * 0.3
        
        # Content metrics (30% weight)
        content_score = min(100, len(text.split()) * 2) * 0.3
        
        total_score = video_score + audio_score + content_score
        return max(0, min(100, total_score))
    
    def _update_live_analysis(self, video_analysis: VideoAnalysisResult):
        """Update live analysis data for UI"""
        
        self.current_analysis.eye_contact_score = video_analysis.eye_contact_score
        self.current_analysis.confidence_level = video_analysis.confidence_score
        self.current_analysis.professionalism_score = video_analysis.professionalism_score
        
        # Calculate overall performance
        self.current_analysis.overall_performance = (
            video_analysis.eye_contact_score * 0.25 +
            video_analysis.confidence_score * 0.25 +
            video_analysis.professionalism_score * 0.25 +
            video_analysis.attention_level * 0.25
        )
        
        # Generate real-time recommendations
        self.current_analysis.recommendations = self._generate_live_recommendations(video_analysis)
    
    def _generate_live_recommendations(self, video_analysis: VideoAnalysisResult) -> List[str]:
        """Generate real-time improvement recommendations"""
        
        recommendations = []
        
        if video_analysis.eye_contact_score < 60:
            recommendations.append("ðŸ‘ï¸ Try to look directly at the camera more often")
        
        if video_analysis.confidence_score < 65:
            recommendations.append("ðŸ˜Š Maintain a confident posture and smile naturally")
        
        if video_analysis.posture_score < 70:
            recommendations.append("ðŸƒ Sit up straight and keep shoulders level")
        
        if video_analysis.professionalism_score < 75:
            recommendations.append("ðŸ’¼ Maintain professional demeanor and appearance")
        
        if len(recommendations) == 0:
            recommendations.append("âœ… Great job! Keep up the excellent performance")
        
        return recommendations
    
    def _add_analysis_overlay(self, frame: np.ndarray) -> np.ndarray:
        """Add real-time analysis overlay to video frame"""
        
        annotated_frame = frame.copy()
        height, width = frame.shape[:2]
        
        # Add semi-transparent overlay panel
        overlay = annotated_frame.copy()
        cv2.rectangle(overlay, (10, 10), (400, 200), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, annotated_frame, 0.3, 0, annotated_frame)
        
        # Add text information
        font = cv2.FONT_HERSHEY_SIMPLEX
        
        # Title
        cv2.putText(annotated_frame, "Live Interview Analysis", (20, 35), font, 0.7, (255, 255, 255), 2)
        
        # Metrics
        cv2.putText(annotated_frame, f"Eye Contact: {self.current_analysis.eye_contact_score:.0f}%", 
                   (20, 65), font, 0.5, (0, 255, 0), 1)
        cv2.putText(annotated_frame, f"Confidence: {self.current_analysis.confidence_level:.0f}%", 
                   (20, 85), font, 0.5, (0, 255, 0), 1)
        cv2.putText(annotated_frame, f"Professionalism: {self.current_analysis.professionalism_score:.0f}%", 
                   (20, 105), font, 0.5, (0, 255, 0), 1)
        cv2.putText(annotated_frame, f"Overall: {self.current_analysis.overall_performance:.0f}%", 
                   (20, 125), font, 0.5, (255, 255, 0), 2)
        
        # Time remaining
        cv2.putText(annotated_frame, f"Time: {self.current_analysis.time_remaining}s", 
                   (20, 155), font, 0.5, (255, 255, 255), 1)
        
        # Performance indicator
        performance_color = (0, 255, 0) if self.current_analysis.overall_performance > 75 else \
                           (0, 255, 255) if self.current_analysis.overall_performance > 50 else (0, 0, 255)
        
        cv2.circle(annotated_frame, (350, 100), 30, performance_color, -1)
        cv2.putText(annotated_frame, f"{self.current_analysis.overall_performance:.0f}", 
                   (335, 107), font, 0.6, (255, 255, 255), 2)
        
        return annotated_frame
    
    def _start_recording(self):
        """Start video recording of the session"""
        
        try:
            # Set up video writer
            fourcc = cv2.VideoWriter_fourcc(*'XVID')
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"Data/Interviews/Sessions/interview_{self.active_session.session_id}_{timestamp}.avi"
            
            self.video_writer = cv2.VideoWriter(filename, fourcc, 20.0, (640, 480))
            self.recording_enabled = True
            self.recording_start_time = datetime.now()
            
            print(f"ðŸŽ¥ Recording started: {filename}")
            
        except Exception as e:
            print(f"âŒ Failed to start recording: {e}")
    
    def _stop_recording(self):
        """Stop video recording"""
        
        if self.video_writer:
            self.video_writer.release()
            self.recording_enabled = False
            
            duration = (datetime.now() - self.recording_start_time).total_seconds()
            print(f"ðŸŽ¥ Recording stopped. Duration: {duration:.1f} seconds")
    
    def _calculate_final_results(self) -> Dict:
        """Calculate final interview results and scores"""
        
        if not self.active_session or not self.active_session.responses:
            return {}
        
        responses = self.active_session.responses
        
        # Calculate average scores
        avg_video_scores = {
            'eye_contact': sum(r.video_analysis.eye_contact_score for r in responses) / len(responses),
            'confidence': sum(r.video_analysis.confidence_score for r in responses) / len(responses),
            'professionalism': sum(r.video_analysis.professionalism_score for r in responses) / len(responses),
            'attention': sum(r.video_analysis.attention_level for r in responses) / len(responses)
        }
        
        avg_audio_scores = {
            'clarity': sum(r.audio_analysis.speech_clarity for r in responses) / len(responses),
            'confidence': sum(r.audio_analysis.voice_confidence for r in responses) / len(responses),
            'pace': sum(r.audio_analysis.speaking_pace for r in responses) / len(responses),
            'filler_words': sum(r.audio_analysis.filler_words_count for r in responses) / len(responses)
        }
        
        # Calculate overall scores
        communication_score = (avg_video_scores['eye_contact'] + avg_audio_scores['clarity']) / 2
        confidence_score = (avg_video_scores['confidence'] + avg_audio_scores['confidence']) / 2
        professionalism_score = avg_video_scores['professionalism']
        technical_score = sum(r.quality_score for r in responses) / len(responses)
        
        overall_score = (
            communication_score * 0.3 +
            confidence_score * 0.25 +
            professionalism_score * 0.2 +
            technical_score * 0.25
        )
        
        return {
            'session_id': self.active_session.session_id,
            'overall_score': overall_score,
            'communication_score': communication_score,
            'confidence_score': confidence_score,
            'professionalism_score': professionalism_score,
            'technical_score': technical_score,
            'detailed_video_scores': avg_video_scores,
            'detailed_audio_scores': avg_audio_scores,
            'total_questions': len(self.active_session.questions),
            'answered_questions': len(responses),
            'session_duration': (self.active_session.end_time - self.active_session.start_time).total_seconds(),
            'improvement_areas': self._identify_improvement_areas(avg_video_scores, avg_audio_scores),
            'strengths': self._identify_strengths(avg_video_scores, avg_audio_scores)
        }
    
    def _identify_improvement_areas(self, video_scores: Dict, audio_scores: Dict) -> List[str]:
        """Identify areas for improvement"""
        
        improvements = []
        
        if video_scores['eye_contact'] < 70:
            improvements.append("Eye contact and camera engagement")
        
        if video_scores['confidence'] < 65:
            improvements.append("Confident body language and facial expressions")
        
        if video_scores['professionalism'] < 75:
            improvements.append("Professional appearance and demeanor")
        
        if audio_scores['clarity'] < 70:
            improvements.append("Speech clarity and articulation")
        
        if audio_scores['filler_words'] > 5:
            improvements.append("Reducing filler words and verbal pauses")
        
        return improvements
    
    def _identify_strengths(self, video_scores: Dict, audio_scores: Dict) -> List[str]:
        """Identify user strengths"""
        
        strengths = []
        
        if video_scores['eye_contact'] >= 80:
            strengths.append("Excellent eye contact and engagement")
        
        if video_scores['confidence'] >= 80:
            strengths.append("Confident and positive demeanor")
        
        if video_scores['professionalism'] >= 85:
            strengths.append("Professional appearance and conduct")
        
        if audio_scores['clarity'] >= 80:
            strengths.append("Clear and articulate communication")
        
        if audio_scores['filler_words'] <= 2:
            strengths.append("Smooth and fluent speech delivery")
        
        return strengths
    
    def _save_session_results(self, results: Dict):
        """Save session results to file"""
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"Data/Interviews/Results/interview_results_{self.active_session.session_id}_{timestamp}.json"
            
            # Add session metadata
            results['session_metadata'] = {
                'user_id': self.active_session.user_id,
                'company_name': self.active_session.company_name,
                'role': self.active_session.role,
                'interview_type': self.active_session.interview_type.value,
                'difficulty': self.active_session.difficulty.value,
                'start_time': self.active_session.start_time.isoformat(),
                'end_time': self.active_session.end_time.isoformat() if self.active_session.end_time else None
            }
            
            with open(filename, 'w') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"ðŸ’¾ Results saved: {filename}")
            
        except Exception as e:
            print(f"âŒ Failed to save results: {e}")
    
    def _pause_interview(self):
        """Pause the current interview"""
        self.session_running = False
        print("â¸ï¸ Interview paused")
    
    def _resume_interview(self):
        """Resume the paused interview"""
        self.session_running = True
        print("â–¶ï¸ Interview resumed")
    
    def _repeat_current_question(self):
        """Repeat the current question"""
        if self.active_session and self.current_analysis.current_question:
            self.engine.speak(f"Let me repeat the question: {self.current_analysis.current_question}")
    
    def _next_question(self):
        """Skip to next question"""
        print("â­ï¸ Moving to next question...")
    
    def _provide_help(self):
        """Provide help during interview"""
        help_msg = "You can say 'pause' to pause the interview, 'repeat' to hear the question again, or 'next' to move to the next question."
        self.engine.speak(help_msg)
    
    def _provide_immediate_feedback(self, response: InterviewResponse):
        """Provide immediate feedback after each response"""
        
        if self.settings.show_live_feedback:
            feedback_parts = []
            
            if response.video_analysis.eye_contact_score > 75:
                feedback_parts.append("Good eye contact")
            
            if response.video_analysis.confidence_score > 75:
                feedback_parts.append("Confident delivery")
            
            if response.audio_analysis.filler_words_count <= 2:
                feedback_parts.append("Clear speech")
            
            if feedback_parts:
                feedback = "Great job! " + ", ".join(feedback_parts) + "."
            else:
                feedback = "Thank you for your response. Let's continue."
            
            print(f"ðŸ’¬ Feedback: {feedback}")
            self.engine.speak(feedback)
    
    def _conduct_closing(self):
        """Conduct interview closing"""
        
        closing_msg = "Thank you for participating in this interview. We've completed all the questions. You'll receive detailed feedback and results shortly."
        self.engine.speak(closing_msg)
        print("ðŸŽ¬ Interview session completed!")

# Initialize the Interview Session Manager
interview_session_manager = InterviewSessionManager()

print("ðŸŽ¬ Real-time Interview Session Manager - READY!")
print("ðŸ“¹ Live camera analysis and recording capabilities")
print("ðŸŽ¤ Real-time audio processing and feedback")
print("ðŸ¤– Interactive AI interviewer with voice commands")
print("ðŸ“Š Live performance metrics and coaching")